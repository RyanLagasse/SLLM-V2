model_name,tuning_method,subset_strategy,subset_size,target_tokens,random_seed,avg_token_length,total_tokens,evaluation_dataset,metrics,training_time
HuggingFaceTB/SmolLM-135M-Instruct,FMT,few_long,179,20250,42,95.25698324022346,17051,MMLU:moral_scenarios,accuracy:0.263,0
HuggingFaceTB/SmolLM-135M-Instruct,FMT,few_long,214,20250,42,94.26168224299066,20172,MMLU:moral_scenarios,accuracy:0.274,0
HuggingFaceTB/SmolLM-135M-Instruct,FMT,few_long,214,20250,42,94.26168224299066,20172,MMLU:moral_scenarios,accuracy:0.274,0
HuggingFaceTB/SmolLM-135M-Instruct,FMT,few_long,214,20250,42,94.26168224299066,20172,MMLU:moral_scenarios,accuracy:0.274,0
HuggingFaceTB/SmolLM-135M-Instruct,FMT,many_short,179,20250,42,78.01117318435755,13964,MMLU:moral_scenarios,accuracy:0.246,0
HuggingFaceTB/SmolLM-135M-Instruct,FMT,many_short,256,20250,42,79.0859375,20246,MMLU:moral_scenarios,accuracy:0.246,0
HuggingFaceTB/SmolLM-135M-Instruct,FMT,many_short,256,20250,42,79.0859375,20246,MMLU:moral_scenarios,accuracy:0.246,0
HuggingFaceTB/SmolLM-135M-Instruct,FMT,many_short,256,20250,42,79.0859375,20246,MMLU:moral_scenarios,accuracy:0.246,0
HuggingFaceTB/SmolLM-135M-Instruct,FMT,balanced,179,20250,42,84.8268156424581,15184,MMLU:moral_scenarios,accuracy:0.268,0
HuggingFaceTB/SmolLM-135M-Instruct,FMT,balanced,239,20250,42,84.65271966527196,20232,MMLU:moral_scenarios,accuracy:0.279,0
HuggingFaceTB/SmolLM-135M-Instruct,FMT,balanced,239,20250,42,84.65271966527196,20232,MMLU:moral_scenarios,accuracy:0.279,0
HuggingFaceTB/SmolLM-135M-Instruct,FMT,balanced,239,20250,42,84.65271966527196,20232,MMLU:moral_scenarios,accuracy:0.279,0
HuggingFaceTB/SmolLM-135M-Instruct,FMT,few_long,179,40500,42,95.25698324022346,17051,MMLU:moral_scenarios,accuracy:0.263,0
HuggingFaceTB/SmolLM-135M-Instruct,FMT,few_long,358,40500,42,91.11173184357541,32618,MMLU:moral_scenarios,accuracy:0.263,0
HuggingFaceTB/SmolLM-135M-Instruct,FMT,few_long,452,40500,42,89.51327433628319,40460,MMLU:moral_scenarios,accuracy:0.251,0
HuggingFaceTB/SmolLM-135M-Instruct,FMT,few_long,452,40500,42,89.51327433628319,40460,MMLU:moral_scenarios,accuracy:0.251,0
HuggingFaceTB/SmolLM-135M-Instruct,FMT,many_short,179,40500,42,78.01117318435755,13964,MMLU:moral_scenarios,accuracy:0.246,0
HuggingFaceTB/SmolLM-135M-Instruct,FMT,many_short,358,40500,42,80.29608938547486,28746,MMLU:moral_scenarios,accuracy:0.263,0
HuggingFaceTB/SmolLM-135M-Instruct,FMT,many_short,494,40500,42,81.94331983805668,40480,MMLU:moral_scenarios,accuracy:0.274,0
HuggingFaceTB/SmolLM-135M-Instruct,FMT,many_short,494,40500,42,81.94331983805668,40480,MMLU:moral_scenarios,accuracy:0.274,0
HuggingFaceTB/SmolLM-135M-Instruct,FMT,balanced,179,40500,42,84.8268156424581,15184,MMLU:moral_scenarios,accuracy:0.268,0
HuggingFaceTB/SmolLM-135M-Instruct,FMT,balanced,358,40500,42,84.55586592178771,30271,MMLU:moral_scenarios,accuracy:0.279,0
HuggingFaceTB/SmolLM-135M-Instruct,FMT,balanced,478,40500,42,84.56694560669456,40423,MMLU:moral_scenarios,accuracy:0.240,0
HuggingFaceTB/SmolLM-135M-Instruct,FMT,balanced,478,40500,42,84.56694560669456,40423,MMLU:moral_scenarios,accuracy:0.240,0
HuggingFaceTB/SmolLM-135M-Instruct,FMT,few_long,179,61364,42,95.25698324022346,17051,MMLU:moral_scenarios,accuracy:0.263,0
HuggingFaceTB/SmolLM-135M-Instruct,FMT,few_long,358,61364,42,91.11173184357541,32618,MMLU:moral_scenarios,accuracy:0.263,0
HuggingFaceTB/SmolLM-135M-Instruct,FMT,few_long,537,61364,42,88.26815642458101,47400,MMLU:moral_scenarios,accuracy:0.257,0
HuggingFaceTB/SmolLM-135M-Instruct,FMT,few_long,716,61364,42,85.70391061452514,61364,MMLU:moral_scenarios,accuracy:0.274,0
HuggingFaceTB/SmolLM-135M-Instruct,FMT,many_short,179,61364,42,78.01117318435755,13964,MMLU:moral_scenarios,accuracy:0.246,0
HuggingFaceTB/SmolLM-135M-Instruct,FMT,many_short,358,61364,42,80.29608938547486,28746,MMLU:moral_scenarios,accuracy:0.263,0
HuggingFaceTB/SmolLM-135M-Instruct,FMT,many_short,537,61364,42,82.5195530726257,44313,MMLU:moral_scenarios,accuracy:0.291,0
HuggingFaceTB/SmolLM-135M-Instruct,FMT,many_short,716,61364,42,85.70391061452514,61364,MMLU:moral_scenarios,accuracy:0.274,0
HuggingFaceTB/SmolLM-135M-Instruct,FMT,balanced,179,61364,42,84.8268156424581,15184,MMLU:moral_scenarios,accuracy:0.268,0
HuggingFaceTB/SmolLM-135M-Instruct,FMT,balanced,358,61364,42,84.55586592178771,30271,MMLU:moral_scenarios,accuracy:0.279,0
HuggingFaceTB/SmolLM-135M-Instruct,FMT,balanced,537,61364,42,84.4804469273743,45366,MMLU:moral_scenarios,accuracy:0.274,0
HuggingFaceTB/SmolLM-135M-Instruct,FMT,balanced,716,61364,42,85.70391061452514,61364,MMLU:moral_scenarios,accuracy:0.274,0
HuggingFaceTB/SmolLM-360M-Instruct,FMT,few_long,179,20250,42,95.25698324022346,17051,MMLU:moral_scenarios,accuracy:0.279,0
HuggingFaceTB/SmolLM-360M-Instruct,FMT,few_long,214,20250,42,94.26168224299066,20172,MMLU:moral_scenarios,accuracy:0.279,0
HuggingFaceTB/SmolLM-360M-Instruct,FMT,few_long,214,20250,42,94.26168224299066,20172,MMLU:moral_scenarios,accuracy:0.279,0
HuggingFaceTB/SmolLM-360M-Instruct,FMT,few_long,214,20250,42,94.26168224299066,20172,MMLU:moral_scenarios,accuracy:0.279,0
HuggingFaceTB/SmolLM-360M-Instruct,FMT,many_short,179,20250,42,78.01117318435755,13964,MMLU:moral_scenarios,accuracy:0.251,0
HuggingFaceTB/SmolLM-360M-Instruct,FMT,many_short,256,20250,42,79.0859375,20246,MMLU:moral_scenarios,accuracy:0.274,0
HuggingFaceTB/SmolLM-360M-Instruct,FMT,many_short,256,20250,42,79.0859375,20246,MMLU:moral_scenarios,accuracy:0.274,0
HuggingFaceTB/SmolLM-360M-Instruct,FMT,many_short,256,20250,42,79.0859375,20246,MMLU:moral_scenarios,accuracy:0.274,0
HuggingFaceTB/SmolLM-360M-Instruct,FMT,balanced,179,20250,42,84.8268156424581,15184,MMLU:moral_scenarios,accuracy:0.279,0
HuggingFaceTB/SmolLM-360M-Instruct,FMT,balanced,239,20250,42,84.65271966527196,20232,MMLU:moral_scenarios,accuracy:0.279,0
HuggingFaceTB/SmolLM-360M-Instruct,FMT,balanced,239,20250,42,84.65271966527196,20232,MMLU:moral_scenarios,accuracy:0.279,0
HuggingFaceTB/SmolLM-360M-Instruct,FMT,balanced,239,20250,42,84.65271966527196,20232,MMLU:moral_scenarios,accuracy:0.279,0
HuggingFaceTB/SmolLM-360M-Instruct,FMT,few_long,179,40500,42,95.25698324022346,17051,MMLU:moral_scenarios,accuracy:0.279,0
HuggingFaceTB/SmolLM-360M-Instruct,FMT,few_long,358,40500,42,91.11173184357541,32618,MMLU:moral_scenarios,accuracy:0.274,0
HuggingFaceTB/SmolLM-360M-Instruct,FMT,few_long,452,40500,42,89.51327433628319,40460,MMLU:moral_scenarios,accuracy:0.274,0
HuggingFaceTB/SmolLM-360M-Instruct,FMT,few_long,452,40500,42,89.51327433628319,40460,MMLU:moral_scenarios,accuracy:0.274,0
HuggingFaceTB/SmolLM-360M-Instruct,FMT,many_short,179,40500,42,78.01117318435755,13964,MMLU:moral_scenarios,accuracy:0.251,0
HuggingFaceTB/SmolLM-360M-Instruct,FMT,many_short,358,40500,42,80.29608938547486,28746,MMLU:moral_scenarios,accuracy:0.302,0
HuggingFaceTB/SmolLM-360M-Instruct,FMT,many_short,494,40500,42,81.94331983805668,40480,MMLU:moral_scenarios,accuracy:0.274,0
HuggingFaceTB/SmolLM-360M-Instruct,FMT,many_short,494,40500,42,81.94331983805668,40480,MMLU:moral_scenarios,accuracy:0.274,0
HuggingFaceTB/SmolLM-360M-Instruct,FMT,balanced,179,40500,42,84.8268156424581,15184,MMLU:moral_scenarios,accuracy:0.279,0
HuggingFaceTB/SmolLM-360M-Instruct,FMT,balanced,358,40500,42,84.55586592178771,30271,MMLU:moral_scenarios,accuracy:0.251,0
HuggingFaceTB/SmolLM-360M-Instruct,FMT,balanced,478,40500,42,84.56694560669456,40423,MMLU:moral_scenarios,accuracy:0.274,0
HuggingFaceTB/SmolLM-360M-Instruct,FMT,balanced,478,40500,42,84.56694560669456,40423,MMLU:moral_scenarios,accuracy:0.274,0
HuggingFaceTB/SmolLM-360M-Instruct,FMT,few_long,179,61364,42,95.25698324022346,17051,MMLU:moral_scenarios,accuracy:0.279,0
HuggingFaceTB/SmolLM-360M-Instruct,FMT,few_long,358,61364,42,91.11173184357541,32618,MMLU:moral_scenarios,accuracy:0.274,0
HuggingFaceTB/SmolLM-360M-Instruct,FMT,few_long,537,61364,42,88.26815642458101,47400,MMLU:moral_scenarios,accuracy:0.274,0
HuggingFaceTB/SmolLM-360M-Instruct,FMT,few_long,716,61364,42,85.70391061452514,61364,MMLU:moral_scenarios,accuracy:0.285,0
HuggingFaceTB/SmolLM-360M-Instruct,FMT,many_short,179,61364,42,78.01117318435755,13964,MMLU:moral_scenarios,accuracy:0.251,0
HuggingFaceTB/SmolLM-360M-Instruct,FMT,many_short,358,61364,42,80.29608938547486,28746,MMLU:moral_scenarios,accuracy:0.302,0
HuggingFaceTB/SmolLM-360M-Instruct,FMT,many_short,537,61364,42,82.5195530726257,44313,MMLU:moral_scenarios,accuracy:0.274,0
HuggingFaceTB/SmolLM-360M-Instruct,FMT,many_short,716,61364,42,85.70391061452514,61364,MMLU:moral_scenarios,accuracy:0.324,0
HuggingFaceTB/SmolLM-360M-Instruct,FMT,balanced,179,61364,42,84.8268156424581,15184,MMLU:moral_scenarios,accuracy:0.279,0
HuggingFaceTB/SmolLM-360M-Instruct,FMT,balanced,358,61364,42,84.55586592178771,30271,MMLU:moral_scenarios,accuracy:0.251,0
HuggingFaceTB/SmolLM-360M-Instruct,FMT,balanced,537,61364,42,84.4804469273743,45366,MMLU:moral_scenarios,accuracy:0.274,0
HuggingFaceTB/SmolLM-360M-Instruct,FMT,balanced,716,61364,42,85.70391061452514,61364,MMLU:moral_scenarios,accuracy:0.274,0
Qwen/Qwen2.5-0.5B-Instruct,FMT,few_long,179,20250,42,95.25698324022346,17051,MMLU:moral_scenarios,accuracy:0.279,0
Qwen/Qwen2.5-0.5B-Instruct,FMT,few_long,214,20250,42,94.26168224299066,20172,MMLU:moral_scenarios,accuracy:0.285,0
Qwen/Qwen2.5-0.5B-Instruct,FMT,few_long,214,20250,42,94.26168224299066,20172,MMLU:moral_scenarios,accuracy:0.285,0
Qwen/Qwen2.5-0.5B-Instruct,FMT,few_long,214,20250,42,94.26168224299066,20172,MMLU:moral_scenarios,accuracy:0.285,0
Qwen/Qwen2.5-0.5B-Instruct,FMT,many_short,179,20250,42,78.01117318435755,13964,MMLU:moral_scenarios,accuracy:0.291,0
Qwen/Qwen2.5-0.5B-Instruct,FMT,many_short,256,20250,42,79.0859375,20246,MMLU:moral_scenarios,accuracy:0.307,0
Qwen/Qwen2.5-0.5B-Instruct,FMT,many_short,256,20250,42,79.0859375,20246,MMLU:moral_scenarios,accuracy:0.307,0
Qwen/Qwen2.5-0.5B-Instruct,FMT,many_short,256,20250,42,79.0859375,20246,MMLU:moral_scenarios,accuracy:0.307,0
Qwen/Qwen2.5-0.5B-Instruct,FMT,balanced,179,20250,42,84.8268156424581,15184,MMLU:moral_scenarios,accuracy:0.285,0
Qwen/Qwen2.5-0.5B-Instruct,FMT,balanced,239,20250,42,84.65271966527196,20232,MMLU:moral_scenarios,accuracy:0.307,0
Qwen/Qwen2.5-0.5B-Instruct,FMT,balanced,239,20250,42,84.65271966527196,20232,MMLU:moral_scenarios,accuracy:0.307,0
Qwen/Qwen2.5-0.5B-Instruct,FMT,balanced,239,20250,42,84.65271966527196,20232,MMLU:moral_scenarios,accuracy:0.307,0
Qwen/Qwen2.5-0.5B-Instruct,FMT,few_long,179,40500,42,95.25698324022346,17051,MMLU:moral_scenarios,accuracy:0.279,0
Qwen/Qwen2.5-0.5B-Instruct,FMT,few_long,358,40500,42,91.11173184357541,32618,MMLU:moral_scenarios,accuracy:0.279,0
